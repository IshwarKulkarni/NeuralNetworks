#
#Copyright (c) Ishwar R. Kulkarni
#All rights reserved.
#
#This file is part of NeuralNetwork Project by
#Ishwar Kulkarni , see https://github.com/IshwarKulkarni/NeuralNetworks
#
#If you so desire, you can copy, redistribute and/or modify this source
#along with  rest of the project. However any copy/redistribution,
#including but not limited to compilation to binaries, must carry
#this header in its entirety. A note must be made about the origin
#of your copy.
#
#NeuralNetwork is being distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#FITNESS FOR A PARTICULAR PURPOSE.

################################################################################


#Configuration for neural netowrk to classify MNIST Handwriting data
#This Config approximates the network config from
# Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, "Gradient-based learning applied 
# to document recognition" in Proceedings of the IEEE, vol. 86, no. 11, Nov 1998
# 
# Except the convolution layer named "Mid" (like all other conv layers)
# works with padded input while the paper above works with convolution
# while paper above uses unpadded input.
# This Config is offten refereed to as LeNet-5 in the literature.

#This network achieves 93% accuracy and .16 means error at the end first epoch 
# with this config; Each epoch takes approx 150seconds.

->NetworkDescription
EtaMultiplier : .5
EtaDecayRate  : 1
ErrorFunction : MeanSquareError
=======
SmallTestRate : 15000
SmallTestSize : 137
->EndNetworkDescription

->ConvLayer # Starts a block of one of more convolution layers
Name	     : Input
IpSize	 	 : [32,32,3]  # Must have an input size for first layer
Activation	 : TanH
NumKernels	 : 6
KernelSize	 : [5,5]
Padded       : 0
->EndConvLayer

->AveragePoolingLayer
Name 		 : Avg1
Activation   : RELU
WindowSize   : [2,2]
->EndAveragePoolingLayer

#->DropConnectLayer
#Name : DCLayer
#DropRate : 25
#->EndDropConnectLayer

#->ConnectionTable
#0,	0,	0,	1,	1,	1
#1,	0,	0,	0,	1,	1
#1,	1,	0,	0,	0,	1
#1,	1,	1,	0,	0,	0
#0,	1,	1,	1,	0,	0
#0,	0,	1,	1,	1,	0
#0,	0,	0,	0,	1,	1
#1,	0,	0,	0,	0,	1
#1,	1,	0,	0,	0,	0
#0,	1,	1,	0,	0,	0
#0,	0,	1,	1,	0,	0
#0,	0,	0,	1,	1,	0
#0,	0,	1,	0,	0,	1
#1,	0,	0,	1,	0,	0
#0,	1,	0,	0,	1,	0
#0,	0,	0,	0,	0,	0
#->EndConnectionTable

->ConvLayer
Name	     : Mid
Activation	 : TanH
NumKernels	 : 16
KernelSize	 : [5,5]
#Padded       : 0
->EndConvLayer

->MaxPoolingLayer
Name 		 : Max1
Activation   : RELU
WindowSize   : [2,2]
->EndMaxPoolingLayer

->ConvLayer
Name	     : LastConv
Activation	 : TanH
NumKernels	 : 15
KernelSize	 : [3,3]
#Padded       : 0
->EndConvLayer

->FullyConnectedLayerGroup
InputSize    : 784
#DropConnect :  25 
#Mid, Sigmoid : 150
Out , TanH : 10
->EndFullyConnectedLayerGroup