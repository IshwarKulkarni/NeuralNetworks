#Andrej Karpathy's Network at https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
#layer_defs = [];
#layer_defs.push({type:'input', out_sx:32, out_sy:32, out_depth:3});
#layer_defs.push({type:'conv', sx:5, filters:16, stride:1, pad:2, activation:'relu'});
#layer_defs.push({type:'pool', sx:2, stride:2});
#layer_defs.push({type:'conv', sx:5, filters:20, stride:1, pad:2, activation:'relu'});
#layer_defs.push({type:'pool', sx:2, stride:2});
#layer_defs.push({type:'conv', sx:5, filters:20, stride:1, pad:2, activation:'relu'});
#layer_defs.push({type:'pool', sx:2, stride:2});
#layer_defs.push({type:'softmax', num_classes:10});
#
#net = new convnetjs.Net();
#net.makeLayers(layer_defs);
#
#trainer = new convnetjs.SGDTrainer(net, {method:'adadelta', batch_size:4, l2_decay:0.0001});


#Dedcribe the network
->NetworkDescription
EtaMultiplier  : .5
EtaDecayRate  : 1
->EndNetworkDescription

->ConvLayer 
Name	 : InputConv
IpSize	 : [32,32,3]
Activation	 : TanH
NumKernels	 : 3
KernelSize	 : [5,5]
KernelStride : [1,1]
Padded       : 0
->EndConvLayer
 
->MaxPoolingLayer #I am assuming "Pooling" is max pooling.
Name       : Max
Activation : RELU
WindowSize : [2,2]
->EndMaxPoolingLayer

->ConvLayer 
Name	     : ConvMid
Activation	 : TanH
NumKernels	 : 3
KernelSize	 : [5,5]
KernelStride : [1,1]
Padded       : 0
->EndConvLayer

->MaxPoolingLayer 
Name       : Max2
Activation : RELU
WindowSize : [2,2]
->EndMaxPoolingLayer

->FullyConnectedLayerGroup
IN , TanH : 20
Out , Sigmoid : 10
->EndFullyConnectedLayerGroup